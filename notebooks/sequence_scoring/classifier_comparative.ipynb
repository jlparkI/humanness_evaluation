{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe908f4-205a-43f0-bb8c-b668a4295fc3",
   "metadata": {},
   "source": [
    "### Comparison with AntiBERTy classifier\n",
    "This notebook compares a mixture model-based classifier with the AntiBERTy LLM-based\n",
    "classifier. Note that in general we prefer not to use classifiers for reasons\n",
    "discussed in Parkinson et al. (accuracy improves, but at the expense of robustness).\n",
    "This comparison is still useful to see whether / if using AntiBERTy provides any\n",
    "benefit compared with use of a much simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934b9323-cb47-4dda-b420-edaae8457030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef as MCC\n",
    "from antpack import SequenceScoringTool\n",
    "\n",
    "if \"notebooks\" in os.getcwd():\n",
    "    os.chdir(os.path.join(\"..\", \"..\"))\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "score_tool = SequenceScoringTool(offer_classifier_option = True)\n",
    "\n",
    "os.chdir(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33a421-2d44-4249-bf64-046998f9cb32",
   "metadata": {},
   "source": [
    "We number and score all the test sequences on the fly using AntPack. There are 450,000 sequences,\n",
    "so this will take a couple minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56850d4a-d6c3-4c55-8a30-c07275888af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_subsample_mouse_balbc.fasta\n",
      "test_subsample_rat.fasta\n",
      "test_subsample_mouse_c576.fasta\n",
      "test_subsample_human.fasta\n",
      "test_subsample_rhesus.fasta\n",
      "test_subsample_mouse_balbc.fasta\n",
      "test_subsample_human.fasta\n",
      "test_subsample_rhesus.fasta\n"
     ]
    }
   ],
   "source": [
    "scoring = { chain:{\"scores\":[], \"species\":[], \"labels\":[]} for chain in [\"heavy\", \"light\"] }\n",
    "\n",
    "for chain in [\"heavy\", \"light\"]:\n",
    "    os.chdir(os.path.join(current_dir, \"train_test_data_immunogenicity_0.0.1\", f\"{chain}_chain\", \"test_sample_sequences\"))\n",
    "    flist = [f for f in os.listdir() if f.endswith(\"fasta\")]\n",
    "\n",
    "    for fname in flist:\n",
    "        if \"camel\" in fname or \"rabbit\" in fname:\n",
    "            continue\n",
    "        print(fname)\n",
    "        with open(fname, \"r\") as fhandle:\n",
    "            seqs = [str(s.seq) for s in SeqIO.parse(fhandle, \"fasta\")]\n",
    "\n",
    "        species = fname.split(\".fasta\")[0].split(\"test_subsample_\")[1]\n",
    "        scoring[chain][\"scores\"] += score_tool.batch_score_seqs(seqs, mode=\"classifier\").tolist()\n",
    "        scoring[chain][\"species\"] += [species for s in seqs]\n",
    "        if species == \"human\":\n",
    "            scoring[chain][\"labels\"] += np.ones((len(seqs))).tolist()\n",
    "        else:\n",
    "            scoring[chain][\"labels\"] += np.zeros((len(seqs))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a19cdf3-6e41-4296-bdce-7a4a777cecf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntPack / SAM scores\n",
      "heavy\n",
      "Accuracy: 0.9987498562334669\n",
      "MCC: 0.9966684585510319\n",
      "\n",
      "light\n",
      "Accuracy: 0.9944602796686899\n",
      "MCC: 0.9876228050179905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"AntPack / SAM scores\")\n",
    "for chain in [\"heavy\", \"light\"]:\n",
    "    preds = np.round(scoring[chain][\"scores\"]).astype(np.int32)\n",
    "    print(chain)\n",
    "    print(f\"Accuracy: {accuracy_score(scoring[chain]['labels'], preds)}\")\n",
    "    print(f\"MCC: {MCC(scoring[chain]['labels'], preds)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbcca13-a32c-4800-bd3a-c879bb55a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntiBERTy heavy chain scores\n",
      "Accuracy: 0.9171554728793812\n",
      "MCC: 0.8169122090301767\n"
     ]
    }
   ],
   "source": [
    "print(\"AntiBERTy heavy chain scores\")\n",
    "\n",
    "os.chdir(os.path.join(current_dir, \"results_and_resources\"))\n",
    "heavy_antiberty = pd.read_csv(\"heavy_antiberty_classifier.txt\", header=None)\n",
    "heavy_antiberty = heavy_antiberty[~heavy_antiberty.iloc[:,0].isin([\"rabbit\", \"camel\"])]\n",
    "heavy_antiberty_labels = [1 if r == \"human\" else 0 for r in heavy_antiberty.iloc[:,0].tolist()]\n",
    "heavy_antiberty_preds = [1 if r == \"Human\" else 0 for r in heavy_antiberty.iloc[:,1].tolist()]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(heavy_antiberty_labels, heavy_antiberty_preds)}\")\n",
    "print(f\"MCC: {MCC(heavy_antiberty_labels, heavy_antiberty_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85c1297-7393-40bb-b66f-61541db0f1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntiBERTy light chain scores\n",
      "Accuracy: 0.9872573019013446\n",
      "MCC: 0.971418079901737\n"
     ]
    }
   ],
   "source": [
    "print(\"AntiBERTy light chain scores\")\n",
    "\n",
    "os.chdir(os.path.join(current_dir, \"results_and_resources\"))\n",
    "light_antiberty = pd.read_csv(\"light_antiberty_classifier.txt\", header=None)\n",
    "light_antiberty_labels = [1 if r == \"human\" else 0 for r in light_antiberty.iloc[:,0].tolist()]\n",
    "light_antiberty_preds = [1 if r == \"Human\" else 0 for r in light_antiberty.iloc[:,1].tolist()]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(light_antiberty_labels, light_antiberty_preds)}\")\n",
    "print(f\"MCC: {MCC(light_antiberty_labels, light_antiberty_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3855950-d13e-43b7-8fbe-dcdc1851563c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
